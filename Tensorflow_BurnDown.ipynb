{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow BurnDown.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOt+mRp7GQz7nyjcSzBpXQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reptilefury/coursera-machine-learning/blob/main/Tensorflow_BurnDown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bYOtd55Hnynq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense,Flatten,Conv2D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare the mnist dataset"
      ],
      "metadata": {
        "id": "-HgS-z6zo9E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "#Split the dataset \n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "#Normalize the data for effective computation\n",
        "x_train,x_test = x_train/255.0,x_test/255.0\n",
        "\n",
        "#Add a channel's dimension\n",
        "x_test = x_test[...,tf.newaxis].astype(\"float32\")\n",
        "x_train = x_train[...,tf.newaxis].astype(\"float32\")\n",
        "\n",
        "#Batch and Shuffle the dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)"
      ],
      "metadata": {
        "id": "9em2JNiVo1vY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image,labels in train_dataset.take(1):\n",
        "  print(image.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFPGUp-1dK5-",
        "outputId": "d7354244-36b2-4352-e4a7-8748f262ca01"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kY1_d8tdY-5",
        "outputId": "ff87ff0a-c242-4a01-9ad8-4ae43a339c9b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fm5DGKvqdkYO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qJq7YOrmdivJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w_ZQZ_Wkdd8N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for images,labels in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3,3, i + 1)  \n",
        "    plt.axis(\"off\")\n",
        "    #plt.imshow(images[i].numpy().astype(\"uint8\"))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "pGCB41T6rcHb",
        "outputId": "3ac9a252-7915-42bd-ebcd-e736069cbfb7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADKUlEQVR4nO3UMQEAIAzAMMC/5+GiHCQKenXPzAKgcV4HAPzEdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIHQBcjcEy3+fc28AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the Keras model using the subclassing API\n",
        "class ModelOne(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(ModelOne,self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(64,3)\n",
        "    self.activ1 = tf.keras.layers.Activation(\"elu\")\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.batchNorm = tf.keras.layers.BatchNormalization()\n",
        "    self.dense1  = tf.keras.layers.Dense(128,activation = \"relu\", kernel_initializer=\"he_normal\")\n",
        "    self.dense2 = tf.keras.layers.Dense(10)\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.activ1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.batchNorm(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    return x \n",
        "\n",
        "\n",
        "\n",
        "#Create an instance of the model\n",
        "model = ModelOne()\n"
      ],
      "metadata": {
        "id": "LjaLoh6stWVO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "egjNRs-3iYqJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the hyperparameters by choosing a loss function to measures the perfomance of the model, choose a learning algorithm/optimizer that will minimize the cost function\n",
        "\n",
        "#Loss function\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam() #Could take a learning rate"
      ],
      "metadata": {
        "id": "f3NB4abgw9ll"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the loss to measure the loss and accuracy of the model "
      ],
      "metadata": {
        "id": "NHiijPXiyCX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gradients_impl import gradients\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_Accuracy\")\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean( name = \"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"training_accuracy\")\n",
        "\n",
        "#Define the training function\n",
        "@tf.function\n",
        "def train_step(images,labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    predictions = model(images,training=True)\n",
        "    loss = loss_fn(labels,predictions)\n",
        "  gradients = tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels,predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(images,labels):\n",
        "  #No gradient checking so we do not utilise Gradient Tape\n",
        "  predictions = model(images,training=False) \n",
        "  loss = loss_fn(labels,predictions)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_accuracy(labels,predictions)"
      ],
      "metadata": {
        "id": "9ElstYyYxvhV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EHsZDeJVkj4m"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the models\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "  #Reset the states\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  #Loops over the training data and pass the images to the training loop\n",
        "  for image,labels in train_dataset:\n",
        "    train_step(image,labels)\n",
        "  for image,labels in test_dataset:\n",
        "    test_step(image,labels)\n",
        "  \n",
        "    print(f\"EPOCHS:{epoch + 1}\\n\")\n",
        "    print(f\"train_loss:{train_loss.result()}\\n\")\n",
        "    print(f\"train_accuracy:{train_accuracy.result()}\")\n",
        "    print(f\"test_loss:{test_loss.result()}\")\n",
        "    print(f\"test_accuracy:{test_accuracy.result()}\")"
      ],
      "metadata": {
        "id": "3jtMyj5YbPJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scratch"
      ],
      "metadata": {
        "id": "Kgq1R-lmk7Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MoIomGwbkkXe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "mnist=tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "#Normalise the data\n",
        "x_train,x_test = x_train/255.0,x_test/255.0\n",
        "\n",
        "#Add a channel\n",
        "x_train = x_train[...,tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[...,tf.newaxis].astype(\"float32\")\n",
        "\n",
        "\n",
        "#Convert to batch \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(1000).batch(32)\n",
        "test_dataset  = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)"
      ],
      "metadata": {
        "id": "vnjKAzgglI_N"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image,labels in train_dataset:\n",
        "  print(image.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHVbHpNZJfrk",
        "outputId": "6fd48e78-c406-4fb7-c63e-ce4c68b782f8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ltFkcScqLEgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kd4OzKIoLAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZmiNtaSoK_NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fdafW3c5K10W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LDsnQVLEKqKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "class NeuralNetwork(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork,self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(64,kernel_size=(2,2),strides=(1,1),padding = \"same\",kernel_initializer=\"he_normal\") \n",
        "    self.activ1 = tf.keras.layers.Activation(\"elu\")\n",
        "    self.maxpool = tf.keras.layers.MaxPooling2D((2,2))\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.batchNorm = tf.keras.layers.BatchNormalization()\n",
        "    self.dense1 = tf.keras.layers.Dense(64,kernel_initializer=\"he_normal\")\n",
        "    self.activ2 = tf.keras.layers.Activation(\"elu\")\n",
        "    self.output1 = tf.keras.layers.Dense(10) #10 classes\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.activ1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.batchNorm(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.activ2(x)\n",
        "    x = self.output1(x)\n",
        "    return x\n",
        "\n",
        "#Create an instance of the model\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "Z6K_R-STohY3"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xd1h2CuCEe_9"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the hyperparameters\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name =\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"testing Accuracy\")\n",
        "\n",
        "\n",
        "\n",
        "#Define the training loop\n",
        "@tf.function\n",
        "def train_step(images,labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #Enable gradient checking\n",
        "    #Get the predictions\n",
        "    predictions = model(images,training=True)\n",
        "    loss = loss_fn(labels,predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss,model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels,predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images,labels):\n",
        "  predictions = model(images,training = False)\n",
        "  loss = loss_fn(labels,predictions)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_accuracy(labels,predictions)"
      ],
      "metadata": {
        "id": "2RuTiypduKxW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "arAm-2fLGLd6"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZV4OAcZfEHTK"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  #Reset the metrics\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  \n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  for images,labels in train_dataset:\n",
        "    train_step(images,labels)\n",
        "\n",
        "  for images, labels in test_dataset:\n",
        "    test_step(images,labels)\n",
        "\n",
        "  print(f\"Epoch: {epoch + 1}\")\n",
        "  print(f\"Training Accuracy:{train_accuracy.result()}\")\n",
        "  print(f\"Train Loss: {train_loss.result()}\")\n",
        "  print(f\"Test Loss: {test_loss.result()}\")\n",
        "  print(f\"Test Accuracy:{test_accuracy.result()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lja1Xal5yf22",
        "outputId": "29057ce5-9ed6-4f3a-b11e-65f6cb693fba"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Training Accuracy:0.9505333304405212\n",
            "Train Loss: 0.17604874074459076\n",
            "Test Loss: 0.9865314364433289\n",
            "Test Accuracy:0.8022000193595886\n",
            "Epoch: 2\n",
            "Training Accuracy:0.9761166572570801\n",
            "Train Loss: 0.0764571875333786\n",
            "Test Loss: 0.13124211132526398\n",
            "Test Accuracy:0.9652000069618225\n",
            "Epoch: 3\n",
            "Training Accuracy:0.9820166826248169\n",
            "Train Loss: 0.05853157863020897\n",
            "Test Loss: 0.31219908595085144\n",
            "Test Accuracy:0.9276000261306763\n",
            "Epoch: 4\n",
            "Training Accuracy:0.9847666621208191\n",
            "Train Loss: 0.05067784711718559\n",
            "Test Loss: 0.1064484566450119\n",
            "Test Accuracy:0.9754999876022339\n",
            "Epoch: 5\n",
            "Training Accuracy:0.9868000149726868\n",
            "Train Loss: 0.04282517358660698\n",
            "Test Loss: 0.22366255521774292\n",
            "Test Accuracy:0.9513999819755554\n",
            "Epoch: 6\n",
            "Training Accuracy:0.9884666800498962\n",
            "Train Loss: 0.036847759038209915\n",
            "Test Loss: 0.15849044919013977\n",
            "Test Accuracy:0.9728000164031982\n",
            "Epoch: 7\n",
            "Training Accuracy:0.9908333420753479\n",
            "Train Loss: 0.029142921790480614\n",
            "Test Loss: 0.12789933383464813\n",
            "Test Accuracy:0.9753000140190125\n",
            "Epoch: 8\n",
            "Training Accuracy:0.9904666543006897\n",
            "Train Loss: 0.031700074672698975\n",
            "Test Loss: 0.11412571370601654\n",
            "Test Accuracy:0.9794999957084656\n",
            "Epoch: 9\n",
            "Training Accuracy:0.9920666813850403\n",
            "Train Loss: 0.025657979771494865\n",
            "Test Loss: 0.12843485176563263\n",
            "Test Accuracy:0.9787999987602234\n",
            "Epoch: 10\n",
            "Training Accuracy:0.9922666549682617\n",
            "Train Loss: 0.02529256045818329\n",
            "Test Loss: 0.12987734377384186\n",
            "Test Accuracy:0.9804999828338623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XgC8s3OhKaAI"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YQskiHeyNgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2gd7zCG4IkPs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JNhVw-z90At-"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RUPMmRTMv1GE"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AYzl5cFruqyw"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PYumJYOErrW1"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}
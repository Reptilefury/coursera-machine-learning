{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro for experts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reptilefury/coursera-machine-learning/blob/main/Intro_for_experts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfNqNgIW-ITr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data set \n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#Split the dataset \n",
        "(train_images,train_labels),(test_images,test_labels) = mnist\n",
        "\n",
        "#Standardize/ normalize the data \n",
        "train_images,test_images = train_images/255.0,test_images/255.0 \n",
        "\n",
        "#Add a channels dimension \n",
        "train_images = train_images[..., tf.newaxis].astype('float32')\n",
        "test_images = test_images[...,tf.newaxis].astype('float32')"
      ],
      "metadata": {
        "id": "No84z6R4AIN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch and shuffle the dataset \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,train_labels)).shuffle(10000).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)"
      ],
      "metadata": {
        "id": "ehooVwUpAffH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model using the subclassing API\n",
        "class MyModel(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel,self).__init__()\n",
        "    self.Conv1 = tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\")\n",
        "    self.MaxPool = tf.keras.layers.MaxPool2D((2,2))\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.Dense1 = tf.keras.layers.Dense(128,activation = \"relu\")\n",
        "    self.Output = tf.keras.layers.Dense(10,)#activation =\"softmax\")\n",
        "  \n",
        "  def call(self,x):\n",
        "    x = self.Conv1(x) \n",
        "    x = self.MaxPool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.Dense1(x)\n",
        "    x = self.Output(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jBgG8kY7CjHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()"
      ],
      "metadata": {
        "id": "TUpYFCbeHdKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the hyperparameters\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "Ui_-rGQnHiEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selects the metrics to measure the loss and accuracy of the model"
      ],
      "metadata": {
        "id": "gvcwtfjzIIKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Metrics to measure the loss and accuracy of the model\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = \"train_accuracy\")\n",
        "\n",
        "#test loss and accuracy \n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test-accuracy\")"
      ],
      "metadata": {
        "id": "OvCIJC03H2gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using tf.Gradient Tape to Train the model \n",
        "@tf.function #Converts a regular python function to a tensorflow graph function \n",
        "def train_step(images,labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #Training = true is there are  layers with different behaviour during training versus inference such as Dropout\n",
        "    predictions = model(images, training =True)\n",
        "    #Calculate the loss \n",
        "    loss = loss_fn(labels,predictions)\n",
        "    gradient = tape.gradient(loss,model.trainable_variables) #Get the gradient of the loss function \n",
        "    optimizer.apply_gradients(zip(gradient, model.trainable_variables)) #Update the parameters based on the gradient of the loss \n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels,predictions)"
      ],
      "metadata": {
        "id": "PinVMMUgKVz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model"
      ],
      "metadata": {
        "id": "rgdEYxnpOG5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  #Training = false is only needed if there are layers with different behaviours\n",
        "  #during training versus inference \n",
        "  predictions = model(images,training=False)\n",
        "  t_loss = loss_fn(predictions,labels)  \n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels,predictions)"
      ],
      "metadata": {
        "id": "hksyHCWOEOTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10 \n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  #Reset the metrics at the start of the next epoch \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  #for image,labels in train_dataset:\n",
        "   # train_step(image,labels)\n",
        "  \n",
        "  for test_images,test_labels in  test_dataset:\n",
        "    test_step(test_images,test_labels)\n",
        "    \n",
        "  print(f\"Epoch:{epoch + 1}/n\",\n",
        "        f\"train_loss {train_loss.result()}/n\",\n",
        "        f\"Accuracy{train_accuracy.result() * 100}/n\",\n",
        "        f\"Test_loss {test_loss.result()}/n\",\n",
        "        f\"Test_accuracy {test_accuracy.result()}/n\"\n",
        "        )\n",
        "  #print(f\"train_loss {train_loss.result()}\")\n",
        "  #print(f\"\")"
      ],
      "metadata": {
        "id": "3UMTgTkUACRN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "00606da6-43d6-4a92-d084-f2d71f010920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a439096ee189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   print(f\"Epoch:{epoch + 1}/n\",\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-9-35b7206c5e0b>\", line 6, in test_step  *\n        t_loss = loss_fn(predictions,labels)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1863, in sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5203, in sparse_categorical_crossentropy\n        labels=target, logits=output)\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(320,) and logits.shape=(1, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D,Flatten,MaxPool2D,Dense,Dropout,BatchNormalization,Activation \n",
        "from tensorflow.nn import relu\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import  GradientTape"
      ],
      "metadata": {
        "id": "2Hf6-jojU_Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and split the dataset \n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "(train_images,train_labels),(test_images,test_labels) = mnist"
      ],
      "metadata": {
        "id": "WWLhplLgUO_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0172513-9b2b-4218-e30a-6c061e4598de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardize the data/ normalize the data\n",
        "train_images = train_images/255.0 #Convert the training data into floating point numbers in between zero and one( train X)\n",
        "test_images = test_images/255.0 #Convert the testx into floating point numbers in between zero and one \n",
        "\n",
        "#Add a channel \n",
        "#train_images = tf.expand_dims(train_images, axis=1) \n",
        "#test_images= test_images[...,tf.newaxis].astype('float32')\n",
        "#test_images = tf.expand_dims(test_images,axis=1)\n",
        "\n",
        "#Add a channel\n",
        "train_images = train_images[...,tf.newaxis].astype('float32')\n",
        "test_images = test_images[...,tf.newaxis].astype('float32')"
      ],
      "metadata": {
        "id": "xW1uhmfqViQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pJRVZk8wkv_",
        "outputId": "25328e20-fdc1-4250-f402-8fd0b45d2ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "61CTYKBVwmmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We batch and shuffle the dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images,train_labels)).shuffle(1000).batch(32)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images,test_labels)).batch(32)"
      ],
      "metadata": {
        "id": "QyWeGwTnvW8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the networ using the  subclassing api\n",
        "class MyModel(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    #First Convolution Block \n",
        "    self.Conv1 = Conv2D(16,(3,3),padding=\"same\",activation =relu,input_shape=train_images[0].shape)\n",
        "    self.MaxPool1 = MaxPool2D((2,2))\n",
        "    self.Drop1 = Dropout(0.2) #Add a dropout layer to prevent overfitting\n",
        "    #Second Convolution Block\n",
        "    self.Conv2 = Conv2D(32,(3,3),padding=\"same\",activation =relu)\n",
        "    self.MaxPool2 = MaxPool2D((2,2))\n",
        "    self.Drop2 = Dropout(0.2) #Another dropout layer to prevent overfitting\n",
        "    \n",
        "    #Third Convolution Block\n",
        "    self.Conv3 = Conv2D(64,(3,3),padding=\"same\",activation =relu)\n",
        "    self.MaxPool3 = MaxPool2D((2,2))\n",
        "    self.Drop3  =  Dropout(0.2) #Another dropout layer to prevent overfitting\n",
        "\n",
        "    #Fourth Convolution Block\n",
        "    self.Conv4 = Conv2D(128,(3,3),padding=\"same\",activation=relu)\n",
        "    self.MaxPool4 = MaxPool2D((2,2))\n",
        "    self.Drop4 = Dropout(0.2)\n",
        "\n",
        "    #Flatten out the learned features into a vector (1D array)\n",
        "    self.flatten = Flatten()\n",
        "    self.BatchNorm1 = BatchNormalization()\n",
        "    self.Dense1 = Dense(300,kernel_initializer=\"he_normal\")\n",
        "    self.Activation1 = Activation('elu')\n",
        "    self.Dense2 = Dense(100,kernel_initializer=\"he_normal\")\n",
        "    self.Activation2 = Activation('elu')\n",
        "    self.BatchNorm2 = BatchNormalization()\n",
        "    self.Output = Dense(10,activation = \"softmax\")\n",
        "\n",
        "  #Calling Function that will be returned when the model will be called\n",
        "  def call(self,x):\n",
        "    #First ConvBlock\n",
        "    x = self.Conv1(x)\n",
        "    x = self.MaxPool1(x)\n",
        "    x = self.Drop1(x)\n",
        "\n",
        "    #Second Convolution Block\n",
        "    x = self.Conv2(x)\n",
        "    x = self.MaxPool2(x)\n",
        "    x = self.Drop2(x)\n",
        "    \n",
        "    #Third Convolution Block\n",
        "    x = self.Conv3(x)\n",
        "    x = self.MaxPool3(x)\n",
        "    #Third Convolution Block\n",
        "    x = self.Conv3(x)\n",
        "    x = self.Drop3(x)\n",
        "\n",
        "    #Fourth Convolution Block\n",
        "    x = self.Conv4(x)\n",
        "    x = self.MaxPool4(x)\n",
        "    x = self.Drop4(x)\n",
        "\n",
        "    #FCN \n",
        "    x = self.flatten(x)\n",
        "    x = self.BatchNorm1(x)\n",
        "    x = self.Dense1(x)\n",
        "    x = self.Activation1(x)\n",
        "    x = self.Dense2(x)\n",
        "    x = self.Activation2(x)\n",
        "    x = self.BatchNorm2(x)\n",
        "    Output = self.Output(x)\n",
        "    return Output "
      ],
      "metadata": {
        "id": "Ijv8fZA3WE8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance of the model \n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "x1i3ZywEhFfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Define the hyperparameters and compile the model\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "learning_rate = 1e-3 \n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "batch_size = 64\n",
        "\n",
        "#Select the metrics to measure the accuracy and loss of the model\n",
        "\n",
        "#Training Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n",
        "#Test Metrics\n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
        "\n",
        "\n",
        "#Cache prefetch and shuffle(Load the data into memory for faster loading after the first epoch)\n",
        "#AUTOTUNE = tf.data.AUTOTUNE\n",
        "#train_images = train_images().cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "FOFOJoX8iG4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uhR5AczzxzzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model and define the training loop \n",
        "#model.compile(loss=loss_fn,optimizer=optimizer)"
      ],
      "metadata": {
        "id": "xkXFtx9JVq5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the callback function and save the model's weights after every five seconds\n",
        "checkpoint = \"expert_model/cp-{epoch:04d}.ckpt\"\n",
        "callback = ModelCheckpoint(checkpoint,verbose = 1,save_weights_only= True,save_freq=5 * batch_size)\n",
        "model.save_weights(checkpoint.format(epoch=0))#String formating to save weights based on the epochs"
      ],
      "metadata": {
        "id": "nTsKyF3Elajm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "@tf.function\n",
        "def train_step(image,label):\n",
        "  with GradientTape() as tape:\n",
        "    predictions = model(image)# make the predictions\n",
        "    tr_loss = loss_fn(label,predictions) #Calculate the loss based on the predictions\n",
        "    #Calculate the gradients based on the loss \n",
        "    gradients = tape.gradient(tr_loss, model.trainable_variables)\n",
        "    #Update the parameters based on the gradients\n",
        "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "    train_loss(tr_loss)\n",
        "    train_accuracy(label,predictions)"
      ],
      "metadata": {
        "id": "GED4qyCWmZxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "@tf.function \n",
        "def test_step(image, label):\n",
        "  with GradientTape() as Tape:\n",
        "    #Make the prediction\n",
        "    predictions = model(image,training=False)#Since we are not going to update the weights we set the training attribute to false \n",
        "    t_loss = loss_fn(label, predictions)\n",
        "\n",
        "    #Metrics of the loss and accuracy\n",
        "    train_loss(t_loss)\n",
        "    train_accuracy(label, predictions)"
      ],
      "metadata": {
        "id": "bW1DIzxTq8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "EXRqYOq7stDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 10\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  #RESET THE METRICS AFTER EVERY EPOCH\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  #RESET THE TESTING METRICS\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  #Iterate through the training data and call the training step function\n",
        "  for image,labels in  train_ds:\n",
        "    train_step(image,labels)\n",
        "  for image,labels in test_ds:\n",
        "    test_step(image,labels)\n",
        "\n",
        "  print(f\"EPOCH: {epoch + 1 }\",\n",
        "        f\"Loss: {train_loss.result()}\",\n",
        "        f\"Accuracy {train_accuracy.result() * 100}\",\n",
        "        f\"Test_Loss: {test_loss.result()}\",\n",
        "        f\"Test_Accuracy: {test_accuracy.result()*100}\")\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-IWhxdJllFC",
        "outputId": "78513525-742b-43ad-9490-30e0043d7ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1 Loss: 0.12897785007953644 Accuracy 95.99142456054688 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 2 Loss: 0.05096712335944176 Accuracy 98.38285827636719 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 3 Loss: 0.03961346298456192 Accuracy 98.76142883300781 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 4 Loss: 0.03221222758293152 Accuracy 99.05428314208984 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 5 Loss: 0.027173558250069618 Accuracy 99.19285583496094 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 6 Loss: 0.023702168837189674 Accuracy 99.24571228027344 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 7 Loss: 0.022063596174120903 Accuracy 99.36714172363281 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 8 Loss: 0.020552359521389008 Accuracy 99.37142944335938 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 9 Loss: 0.01816730760037899 Accuracy 99.48999786376953 Test_Loss: 0.0 Test_Accuracy: 0.0\n",
            "EPOCH: 10 Loss: 0.016401035711169243 Accuracy 99.52285766601562 Test_Loss: 0.0 Test_Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F9na61tYzOQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eOylqO5JvJU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZFllCdP8vFBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LjK-hNRmtMm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H3y5kfwhtK4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5Po3KR2ItCZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ylTXeyAXs_9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XY4tsR8wsfSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VsehlrY3sLat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VMesBftYsLjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YoMaGstClk_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
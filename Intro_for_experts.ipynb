{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro for experts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reptilefury/coursera-machine-learning/blob/main/Intro_for_experts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfNqNgIW-ITr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data set \n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#Split the dataset \n",
        "(train_images,train_labels),(test_images,test_labels) = mnist\n",
        "\n",
        "#Standardize/ normalize the data \n",
        "train_images,test_images = train_images/255.0,test_images/255.0 \n",
        "\n",
        "#Add a channels dimension \n",
        "train_images = train_images[..., tf.newaxis].astype('float32')\n",
        "test_images = test_images[...,tf.newaxis].astype('float32')"
      ],
      "metadata": {
        "id": "No84z6R4AIN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch and shuffle the dataset \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,train_labels)).shuffle(10000).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)"
      ],
      "metadata": {
        "id": "ehooVwUpAffH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model using the subclassing API\n",
        "class MyModel(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel,self).__init__()\n",
        "    self.Conv1 = tf.keras.layers.Conv2D(64,(3,3),activation = \"relu\")\n",
        "    self.MaxPool = tf.keras.layers.MaxPool2D((2,2))\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.Dense1 = tf.keras.layers.Dense(128,activation = \"relu\")\n",
        "    self.Output = tf.keras.layers.Dense(10,)#activation =\"softmax\")\n",
        "  \n",
        "  def call(self,x):\n",
        "    x = self.Conv1(x) \n",
        "    x = self.MaxPool(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.Dense1(x)\n",
        "    x = self.Output(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jBgG8kY7CjHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()"
      ],
      "metadata": {
        "id": "TUpYFCbeHdKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the hyperparameters\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "Ui_-rGQnHiEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selects the metrics to measure the loss and accuracy of the model"
      ],
      "metadata": {
        "id": "gvcwtfjzIIKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Metrics to measure the loss and accuracy of the model\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = \"train_accuracy\")\n",
        "\n",
        "#test loss and accuracy \n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test-accuracy\")"
      ],
      "metadata": {
        "id": "OvCIJC03H2gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using tf.Gradient Tape to Train the model \n",
        "@tf.function #Converts a regular python function to a tensorflow graph function \n",
        "def train_step(images,labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #Training = true is there are  layers with different behaviour during training versus inference such as Dropout\n",
        "    predictions = model(images, training =True)\n",
        "    #Calculate the loss \n",
        "    loss = loss_fn(labels,predictions)\n",
        "    gradient = tape.gradient(loss,model.trainable_variables) #Get the gradient of the loss function \n",
        "    optimizer.apply_gradients(zip(gradient, model.trainable_variables)) #Update the parameters based on the gradient of the loss \n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels,predictions)"
      ],
      "metadata": {
        "id": "PinVMMUgKVz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model"
      ],
      "metadata": {
        "id": "rgdEYxnpOG5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  #Training = false is only needed if there are layers with different behaviours\n",
        "  #during training versus inference \n",
        "  predictions = model(images,training=False)\n",
        "  t_loss = loss_fn(predictions,labels)  \n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels,predictions)"
      ],
      "metadata": {
        "id": "hksyHCWOEOTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10 \n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  #Reset the metrics at the start of the next epoch \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  #for image,labels in train_dataset:\n",
        "   # train_step(image,labels)\n",
        "  \n",
        "  for test_images,test_labels in  test_dataset:\n",
        "    test_step(test_images,test_labels)\n",
        "    \n",
        "  print(f\"Epoch:{epoch + 1}/n\",\n",
        "        f\"train_loss {train_loss.result()}/n\",\n",
        "        f\"Accuracy{train_accuracy.result() * 100}/n\",\n",
        "        f\"Test_loss {test_loss.result()}/n\",\n",
        "        f\"Test_accuracy {test_accuracy.result()}/n\"\n",
        "        )\n",
        "  #print(f\"train_loss {train_loss.result()}\")\n",
        "  #print(f\"\")"
      ],
      "metadata": {
        "id": "3UMTgTkUACRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv2D,Flatten,MaxPool2D,Dense,Dropout,BatchNormalization,Activation \n",
        "from tensorflow.nn import relu\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import  GradientTape"
      ],
      "metadata": {
        "id": "2Hf6-jojU_Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and split the dataset \n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "(train_images,train_labels),(test_images,test_labels) = mnist"
      ],
      "metadata": {
        "id": "WWLhplLgUO_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardize the data/ normalize the data\n",
        "train_images = train_images/255.0 #Convert the training data into floating point numbers in between zero and one( train X)\n",
        "test_images = test_images/255.0 #Convert the testx into floating point numbers in between zero and one \n",
        "\n",
        "#Add a channel \n",
        "#train_images = tf.expand_dims(train_images, axis=1) \n",
        "#test_images= test_images[...,tf.newaxis].astype('float32')\n",
        "#test_images = tf.expand_dims(test_images,axis=1)\n",
        "\n",
        "#Add a channel\n",
        "train_images = train_images[...,tf.newaxis].astype('float32')\n",
        "test_images = test_images[...,tf.newaxis].astype('float32')"
      ],
      "metadata": {
        "id": "xW1uhmfqViQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0].shape"
      ],
      "metadata": {
        "id": "6pJRVZk8wkv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "61CTYKBVwmmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We batch and shuffle the dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images,train_labels)).shuffle(1000).batch(32)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images,test_labels)).batch(32)"
      ],
      "metadata": {
        "id": "QyWeGwTnvW8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the networ using the  subclassing api\n",
        "class MyModel(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    #First Convolution Block \n",
        "    self.Conv1 = Conv2D(16,(3,3),padding=\"same\",activation =relu,input_shape=train_images[0].shape)\n",
        "    self.MaxPool1 = MaxPool2D((2,2))\n",
        "    self.Drop1 = Dropout(0.2) #Add a dropout layer to prevent overfitting\n",
        "    #Second Convolution Block\n",
        "    self.Conv2 = Conv2D(32,(3,3),padding=\"same\",activation =relu)\n",
        "    self.MaxPool2 = MaxPool2D((2,2))\n",
        "    self.Drop2 = Dropout(0.2) #Another dropout layer to prevent overfitting\n",
        "    \n",
        "    #Third Convolution Block\n",
        "    self.Conv3 = Conv2D(64,(3,3),padding=\"same\",activation =relu)\n",
        "    self.MaxPool3 = MaxPool2D((2,2))\n",
        "    self.Drop3  =  Dropout(0.2) #Another dropout layer to prevent overfitting\n",
        "\n",
        "    #Fourth Convolution Block\n",
        "    self.Conv4 = Conv2D(128,(3,3),padding=\"same\",activation=relu)\n",
        "    self.MaxPool4 = MaxPool2D((2,2))\n",
        "    self.Drop4 = Dropout(0.2)\n",
        "\n",
        "    #Flatten out the learned features into a vector (1D array)\n",
        "    self.flatten = Flatten()\n",
        "    self.BatchNorm1 = BatchNormalization()\n",
        "    self.Dense1 = Dense(300,kernel_initializer=\"he_normal\")\n",
        "    self.Activation1 = Activation('elu')\n",
        "    self.Dense2 = Dense(100,kernel_initializer=\"he_normal\")\n",
        "    self.Activation2 = Activation('elu')\n",
        "    self.BatchNorm2 = BatchNormalization()\n",
        "    self.Output = Dense(10,activation = \"softmax\")\n",
        "\n",
        "  #Calling Function that will be returned when the model will be called\n",
        "  def call(self,x):\n",
        "    #First ConvBlock\n",
        "    x = self.Conv1(x)\n",
        "    x = self.MaxPool1(x)\n",
        "    x = self.Drop1(x)\n",
        "\n",
        "    #Second Convolution Block\n",
        "    x = self.Conv2(x)\n",
        "    x = self.MaxPool2(x)\n",
        "    x = self.Drop2(x)\n",
        "    \n",
        "    #Third Convolution Block\n",
        "    x = self.Conv3(x)\n",
        "    x = self.MaxPool3(x)\n",
        "    #Third Convolution Block\n",
        "    x = self.Conv3(x)\n",
        "    x = self.Drop3(x)\n",
        "\n",
        "    #Fourth Convolution Block\n",
        "    x = self.Conv4(x)\n",
        "    x = self.MaxPool4(x)\n",
        "    x = self.Drop4(x)\n",
        "\n",
        "    #FCN \n",
        "    x = self.flatten(x)\n",
        "    x = self.BatchNorm1(x)\n",
        "    x = self.Dense1(x)\n",
        "    x = self.Activation1(x)\n",
        "    x = self.Dense2(x)\n",
        "    x = self.Activation2(x)\n",
        "    x = self.BatchNorm2(x)\n",
        "    Output = self.Output(x)\n",
        "    return Output "
      ],
      "metadata": {
        "id": "Ijv8fZA3WE8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance of the model \n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "x1i3ZywEhFfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Define the hyperparameters and compile the model\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "learning_rate = 1e-3 \n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "batch_size = 64\n",
        "\n",
        "#Select the metrics to measure the accuracy and loss of the model\n",
        "\n",
        "#Training Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n",
        "#Test Metrics\n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
        "\n",
        "\n",
        "#Cache prefetch and shuffle(Load the data into memory for faster loading after the first epoch)\n",
        "#AUTOTUNE = tf.data.AUTOTUNE\n",
        "#train_images = train_images().cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "FOFOJoX8iG4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uhR5AczzxzzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model and define the training loop \n",
        "#model.compile(loss=loss_fn,optimizer=optimizer)"
      ],
      "metadata": {
        "id": "xkXFtx9JVq5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the callback function and save the model's weights after every five seconds\n",
        "checkpoint = \"expert_model/cp-{epoch:04d}.ckpt\"\n",
        "callback = ModelCheckpoint(checkpoint,verbose = 1,save_weights_only= True,save_freq=5 * batch_size)\n",
        "model.save_weights(checkpoint.format(epoch=0))#String formating to save weights based on the epochs"
      ],
      "metadata": {
        "id": "nTsKyF3Elajm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "@tf.function\n",
        "def train_step(image,label):\n",
        "  with GradientTape() as tape:\n",
        "    predictions = model(image)# make the predictions\n",
        "    tr_loss = loss_fn(label,predictions) #Calculate the loss based on the predictions\n",
        "    #Calculate the gradients based on the loss \n",
        "    gradients = tape.gradient(tr_loss, model.trainable_variables)\n",
        "    #Update the parameters based on the gradients\n",
        "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "\n",
        "    train_loss(tr_loss)\n",
        "    train_accuracy(label,predictions)"
      ],
      "metadata": {
        "id": "GED4qyCWmZxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "@tf.function \n",
        "def test_step(image, label):\n",
        "    #Make the prediction\n",
        "  predictions = model(image,training=False)#Since we are not going to update the weights we set the training attribute to false \n",
        "  t_loss = loss_fn(label, predictions)\n",
        "\n",
        "    #Metrics of the loss and accuracy\n",
        "  train_loss(t_loss)\n",
        "  train_accuracy(label, predictions)"
      ],
      "metadata": {
        "id": "bW1DIzxTq8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "EXRqYOq7stDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_ds)"
      ],
      "metadata": {
        "id": "45b1E7Q92OUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tw8Y1iYe2qVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "feFk0ioQ2fNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ukExEVDa2bD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EYF3xawT2UCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5WZCQGEH2R82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 10\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  #RESET THE METRICS AFTER EVERY EPOCH\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  #RESET THE TESTING METRICS\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  #Iterate through the training data and call the training step function\n",
        "  for image,labels in  train_ds:\n",
        "    train_step(image,labels)\n",
        "  for image,labels in test_ds:\n",
        "    test_step(image,labels)\n",
        "  #for batch,(X,y) in train_ds:\n",
        "   # test_step(X,y)\n",
        "\n",
        "  print(f\"EPOCH: {epoch + 1 }\",\n",
        "        f\"Loss: {train_loss.result()}\",\n",
        "        f\"Accuracy {train_accuracy.result() * 100}\",\n",
        "        f\"Test_Loss: {test_loss.result()}\",\n",
        "        f\"Test_Accuracy: {test_accuracy.result()*100}\")\n",
        " "
      ],
      "metadata": {
        "id": "a-IWhxdJllFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlUVQJS010wI",
        "outputId": "c33dfa73-be65-4439-cf41-1acd12a5b41f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Flatten,Dense,Conv2D,MaxPool2D,BatchNormalization,Dropout,Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "9wX2kfbX385r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "(train_images,train_labels),(test_images,validation_images) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "YoMaGstClk_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f761ca5d-5ae2-41ef-c6bf-58a6b664444a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(test_images,test_labels) = train_test_split(test_images,random_state =42)"
      ],
      "metadata": {
        "id": "a82x2d5u-g94"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OQ6Hy6MhWX7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(validation_images,validation_labels) = train_test_split(validation_images,random_state=42)"
      ],
      "metadata": {
        "id": "ngwyH06B-xj7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preprocesssing\n",
        "train_images = train_images/255.0\n",
        "validation_images = validation_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "#Add a channel\n",
        "#train_images = train_images[...,tf.newaxis].astype('float32')\n",
        "#validation_images = validation_images[...,tf.newaxis].astype('float32')\n",
        "#test_images = test_images[...,tf.newaxis].astype('float32')\n",
        "train_images = tf.expand_dims(train_images,0)\n",
        "validation_images = tf.expand_dims(validation_images,0)\n",
        "test_images = tf.expand_dims(test_images,0)"
      ],
      "metadata": {
        "id": "fejpdQoOVpIy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdivOfHzXTSh",
        "outputId": "503895d7-b5b9-4db5-e7f8-4ea726723519"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 1, 60000, 28, 28, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PlZlhxHjXWqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lyEhAFhdXR3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0tnoLgz5XF7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WkHtUW2RWqlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has been split into the ration 80:10:10"
      ],
      "metadata": {
        "id": "OGPbq6XzAZvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the size of the split data before proceeding\n",
        "print(f\"Train_images:{len(train_images)}\")\n",
        "print(f\"train_labels:{len(train_labels)}\")\n",
        "print(f\"Test_images:{len(test_images)}\")\n",
        "print(f\"test_labels:{len(test_labels)}\")\n",
        "print(f\"validation_images:{len(validation_images)}\")\n",
        "print(f\"validation_labels: {len(validation_labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZqp2eX_Uxz",
        "outputId": "9a1f029d-b1e8-4d50-cc19-e43d784c6eda"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_images:60000\n",
            "train_labels:60000\n",
            "Test_images:7500\n",
            "test_labels:2500\n",
            "validation_images:7500\n",
            "validation_labels: 2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We visualize the dataset labels to check if everything is labeled correctly \n",
        "class_names = [\"Top/T-shirt\"\n",
        "\"Trouser\",\n",
        "\"PullOver\",\n",
        "\"Dress\",\n",
        "\"Coat\",\n",
        "\"Sandal\",\n",
        "\"Shirt\",\n",
        "\"Sneaker\"\n",
        ",\"Bag\",\n",
        "\"Ankle boot\"]"
      ],
      "metadata": {
        "id": "NJzwuQBFAS5o"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "rows,cols = 3,3\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "for image in train_images:\n",
        "  for label in train_labels:\n",
        "    for i in range(9):\n",
        "      figure.add_subplot(rows,cols,i+1) \n",
        "      plt.axis('off')\n",
        "      plt.imshow(image,cmap =\"gray\")\n",
        "#      plt.title(class_names[label])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HVD58jJLBdDX",
        "outputId": "32ebe616-d998-43d4-ae45-2a305c7d79b1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport matplotlib.pyplot as plt\\nrows,cols = 3,3\\nfigure = plt.figure(figsize=(10,10))\\nfor image in train_images:\\n  for label in train_labels:\\n    for i in range(9):\\n      figure.add_subplot(rows,cols,i+1) \\n      plt.axis(\\'off\\')\\n      plt.imshow(image,cmap =\"gray\")\\n#      plt.title(class_names[label])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nYxiY_pYSr1F",
        "outputId": "18094d1a-1d8f-4f33-940d-861372cd585b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the computational graph \n",
        "class MyModel(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel,self).__init__()\n",
        "    #First Convolution Block\n",
        "    self.Conv1 = Conv2D(16,(3,3),padding=\"same\",activation = \"relu\", input_shape =train_images[0].shape )\n",
        "    self.MaxPool1 = MaxPool2D((2,2))\n",
        "    self.Drop1 = Dropout(0.2)\n",
        "    \n",
        "    #Second Convolution Block\n",
        "    self.Conv2 = Conv2D(32,(3,3),padding = \"same\",activation=\"relu\")\n",
        "    self.MaxPool2 = MaxPool2D((2,2))\n",
        "    self.Drop2 = Dropout(0.2)\n",
        "\n",
        "    #Third Convolution Block \n",
        "    self.Conv3 = Conv2D(64,(3,3),padding =\"same\",activation = \"relu\")\n",
        "    self.MaxPool3 = MaxPool2D((2,2))\n",
        "    self.Drop3 = Dropout(0.2)\n",
        "\n",
        "    #Fourth Convolution Block\n",
        "    self.Conv4 = Conv2D(128,(3,3), padding =\"same\",activation = \"relu\")\n",
        "    self.MaxPool4 = MaxPool2D((2,2))\n",
        "    self.Drop4 = Dropout(0.2)\n",
        "\n",
        "    #FCN\n",
        "    self.flatten = Flatten()\n",
        "    self.BatchNorm1 = BatchNormalization()\n",
        "    self.Dense1 = Dense(300,kernel_initializer=\"he_normal\")\n",
        "    self.Activation1 = Activation(\"elu\")\n",
        "    self.Dense2 = Dense(100,kernel_initializer = \"he_normal\")    \n",
        "    self.Activation2 = Activation(\"elu\")\n",
        "    self.BatchNorm2  = BatchNormalization()\n",
        "    self.Output = Dense(10,activation=\"softmax\")\n",
        "\n",
        "  def call(self,x):\n",
        "    #First Conv Block\n",
        "    convol1 = self.Conv1(x)\n",
        "    maxP1 = self.MaxPool1(convol1)\n",
        "    drop1 = self.Drop1(maxP1)\n",
        "\n",
        "    #Second Conv Block\n",
        "    conv2 =  self.Conv2(drop1)\n",
        "    maxP2 = self.MaxPool2(conv2)\n",
        "    drop2 = self.Drop2(maxP2)\n",
        "\n",
        "    #Third Conv Block\n",
        "    conv3 = self.Conv3(drop2)\n",
        "    maxP3 = self.MaxPool3(conv3)\n",
        "    drop3 = self.Drop3(maxP3)\n",
        "\n",
        "    #Fourth Conv Block\n",
        "    conv4 = self.Conv4(drop3)\n",
        "    maxP4 = self.MaxPool4(conv4)\n",
        "    drop4 = self.Drop4(maxP4)\n",
        "\n",
        "\n",
        "    #FCN\n",
        "    flatten = self.flatten(drop4)\n",
        "    batchNorm1 = self.BatchNorm1(flatten)\n",
        "    dense1 = self.Dense1(batchNorm1)\n",
        "    activation1 = self.Activation1(dense1)\n",
        "    dense2 = self.Dense2(activation1)\n",
        "    activation2 = self.Activation2(dense2)\n",
        "    batchNorm2 = self.BatchNorm2(activation2)\n",
        "    return self.Output(batchNorm2)"
      ],
      "metadata": {
        "id": "vZyBhBzuC6s1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E3FMXESyVFN6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jAhUMs1qVVor"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Evs4yzU6VUB6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q5I8qyAVVTQ6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A3MuA6yVTu9u"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance of the model\n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "s55Mhe3TJPCD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the hyperparameters\n",
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "EPOCHS = 50 \n",
        "\n",
        "#Metrics \n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name =\"train_accuracy\")\n",
        "\n",
        "#Test Metrics\n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")"
      ],
      "metadata": {
        "id": "s18-7ypU-cnx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nnjadxAnOdL-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oFr-2NkjMCWf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the training loop function\n",
        "def train_step(image, label):\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    #Forward pass \n",
        "    prediction = model(image)\n",
        "    tr_loss = loss_fn(label,prediction)\n",
        "\n",
        "    #Backward Pass\n",
        "    gradients  = tape.gradient(tr_loss, model.variables)#Calculate the gradient of the loss with respect to specific weight parameters\n",
        "    optimizer.apply_gradients(zip(gradients,model.variables)) #Update the gradients\n",
        "\n",
        "\n",
        "    train_loss(train_loss)\n",
        "    train_accuracy(prediction,train_loss)"
      ],
      "metadata": {
        "id": "xVxWyAJaLfHD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(image, label):\n",
        "\n",
        "  prediction = model(image)\n",
        "  ts_loss = loss_fn(label,prediction)#Calculate the loss which is the distance between the models predictions and the target label\n",
        "\n",
        "  test_loss(ts_loss)\n",
        "  test_accuracy(prediction,ts_loss)"
      ],
      "metadata": {
        "id": "W3acKMeONbCo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in  range(EPOCHS):\n",
        "  train_accuracy.reset_states()\n",
        "  train_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  for image_x in train_images:\n",
        "    for image_y in train_labels:\n",
        "      train_step(image_x,image_y)\n",
        "\n",
        "  for image_x in validation_images:\n",
        "    for image_y in validation_labels:\n",
        "      test_step(image_x,image_y)\n",
        "\n",
        "\n",
        "    validation_labels "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "UXw2Hja6OOZM",
        "outputId": "b18f6e2b-6ea9-4e83-80d1-4fd407336af6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f97f342d69c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimage_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimage_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-1b84f3172216>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(image, label)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-3ea651064575>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#First Conv Block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mconvol1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmaxP1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvol1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdrop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDrop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxP1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"my_model_2\" (type MyModel).\n\nInput 0 of layer \"max_pooling2d_8\" is incompatible with the layer: expected ndim=4, found ndim=6. Full shape received: (28, 28, 1, 1, 1, 16)\n\nCall arguments received:\n  • x=tf.Tensor(shape=(28, 28, 1, 1, 1, 1), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uFEdKyTrTWEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e4BsmrcSS9zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RyhKVkLQPA3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D8REwKT_O39W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JbRZQPiQOsvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "292cbDdINICP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LcyspV5mKq-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9fXQmFGyKfph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nXY1Ck9jKZL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}